{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Data analysis theoretical questions and tasks for colloquium and answers for them*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Expected and empirical risk minimization. Discriminant functions. Write out discriminant functions for multiclass linear classifier and K-NN.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый риск** определяется как математическое ожидание функции потерь:\n",
    "$$\n",
    "\\mathit{\\int}\\int\\mathit{\\mathcal{L}(f_{\\theta}(\\mathbf{x}),y) \\cdot p(\\mathbf{x},y)d\\mathbf{x}dy\\to\\min_{\\theta}}\n",
    "$$\n",
    "В общем случае ожидаемый риск не может быть вычислен, поскольку распределение $p(x, y)$ неизвестно для обучающего алгоритма. Однако мы можем вычислить приближение, называемое **эмпирическим риском**, путём усреднения функции потерь на тренировочном множестве:\n",
    "$$\n",
    "L(\\theta|X,Y)=\\frac{1}{N}\\sum_{n=1}^{N}\\mathcal{L}(f_{\\theta}(\\mathbf{x}_{n}),\\,y_{n})\n",
    "$$\n",
    "*Метод минимизации эмпирического риска* заключается в выборе модели, минимизирующей *эмпирический риск*:\n",
    "$$\n",
    "\\widehat{\\theta}=\\arg\\min_{\\theta}L(\\theta|X,Y)\n",
    "$$\n",
    "**Discriminant function** - функция, с помощью которой принимается решения в задачах классификации.\n",
    "$$\n",
    "c=\\arg\\min_{i}g_c \\text{ или } c=\\arg\\max_{i}g_c\n",
    "$$\n",
    "*Discriminant function* для многоклассовой линейной классификации:\n",
    "$$\n",
    "g_{c}(x)=w_{c}^{T}x+w_{c0}\n",
    "$$\n",
    "Тогда:\n",
    "$$\n",
    "\\widehat{y}(x)=\\arg\\max_{c}g_{c}(x)\n",
    "$$\n",
    "*Discriminant function* для K-NN\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_{c}(x) & =\\sum_{k=1}^{K}\\mathbb{I}[y_{i_{k}}=c],\\quad c=1,2,...C.\\\\\n",
    "\\widehat{y}(x) & =\\arg\\max_{c}g_{c}(x)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Describe model evaluation with train/test sets, cross validation and leave-one-out techniques. Over-fitting and under-fitting. How expected train/test loss changes with train set size and complexity of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки моделей с помощью деления на **train/test** наборы, мы делим объекты на две группы и обучаемся на *train*, а оцениваем модель на *test* наборе, тем самым проверяя обобщающую способность модели.\n",
    "\n",
    "При **кросс-валидации** выборка делится на $K$ частей. И $K$ раз одну часть берем для оценки, а остальные для обучения.\n",
    "\n",
    "<img src=\"img/cross_validation.png\" width=\"500\">\n",
    "\n",
    "Итоговая оценка модели вычисляется как среднее арифметическое всех оценок:\n",
    "\n",
    "$$\n",
    "\\widehat{L}_{total}=\\frac{1}{N}\\sum_{n=1}^{N}\\mathcal{L}(f_{\\widehat{\\theta}^{-k(n)}}(x_{n}),\\,y_{n})\n",
    "$$\n",
    "\n",
    "Когда $K=N$, то это называется **leave-one-out**\n",
    "\n",
    "**Переобучение** - случай, когда модель очень сильно подстраивается под обуччающий набор данных и имеет плохую обобщающую способность\n",
    "**Недообучение** - случай, при котором модель оказывается слишком простой, чтобы отображать некоторый *закон природы*\n",
    "\n",
    "<img src=\"img/o_and_ufitting.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is one-hot-encoding? Give feature normalization methods. Why all these feature transformations are important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-hot-encoding** - способ представления категориальных признаков, при котором один категориальный признак разбивается на несколько бинарных (на один меньше, чем значений), каждый из которых отвечает за одно значение первоначального признака. Он необходим для коректного интерпретации категориальных признаков моделью. \n",
    "\n",
    "**Методы нормализации признаков**:\n",
    "1. *Minmax-нормализация*\n",
    "$$\n",
    "x'= \\frac{x-min(x)}{max(x)-min(x)}\n",
    "$$\n",
    "2. *Стандартизация*\n",
    "$$\n",
    "x' = \\frac{x-\\overline{x}}{\\sigma}\n",
    "$$\n",
    "\n",
    "Данные методы необходимы в случаях, когда алгоритмы машинного обучения не работают верно без нормализации. Например, классификаторы могут считать Евклидово расстояние между двумя объектами, и если один из признаков будет другого масштаба, то расстояние будет во многом зависить от этого признака.\n",
    "Так же градиетный спуск работает быстрее при отнормированных признаках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How do K-NN and decision tree methods change when they are applied in classification and in regression context?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-NN**:\n",
    "* Классификация: среди $K$ ближайших соседей выбирается преобладающий класс;\n",
    "* Регрессия: среди $K$ ближайших берется среднее значение.  \n",
    "\n",
    "**Решающее дерево**:\n",
    "* Классификация: при обучении в листьях дерева выбирается преобладающий класс;\n",
    "* Регрессия: при обучении в листьях дерева берется среднее значение попавших туда объектов.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Give definition of the following methods and explain why or why not feature scaling can affect their performance: linear regression estimated with least squares minimization, linear regression with regularization, CART decision trees, logistic regression without regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Линейная регрессия с МНК** - модель регрессии, которая стремится минимизировать сумма квадратов разности предсказанных значений и действительных:\n",
    "$$\n",
    "L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2N}\\sum^{N}_{n=1}(\\hat{y}_{n} - y_{n})^2 = \\frac{1}{2N}\\sum^{N}_{n=1}\\left(\\sum_{d=0}^{D}\\beta_{d}x_{n}^{d}-y_{n}\\right)^{2}  \\rightarrow \\min\\limits_{\\beta}\n",
    "$$\n",
    "Если матрица $X^\\top X$ невырождена и выборка не очень большая, то решается данная задача так:\n",
    "$$\n",
    "\\beta = (X^\\top X)^{-1} X^\\top y\n",
    "$$\n",
    "*Масштабирование* ограничит влияние определенных признаков.\n",
    "\n",
    "2. **Линейная регрессия с регуляризацией**\n",
    "Регуляризация - техника, которая уменьшает сложность модели во избежание переобучения. В общем виде:\n",
    "$$\n",
    "\\sum_{n=1}^{N}\\left(x_{n}^{T}\\beta-y_{n}\\right)^{2}+\\lambda R(\\beta)\\to\\min_{\\beta}\n",
    "$$\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{Lasso regression} & R(\\beta)=||\\beta||_{1} \\\\\n",
    "\\text{Ridge regression} & R(\\beta)=||\\beta||_{2}^{2} \\\\\n",
    "\\text{Elastic net} & R(\\beta)=\\alpha||\\beta||_{1}+(1-\\alpha)||\\beta||_{2}^{2}\\to\\min_{\\beta} \n",
    "\\end{array}\n",
    "$$\n",
    "*Масштабирование* ускорит градиентный спуск.\n",
    "\n",
    "3. **CART деревья** - это бинарные деревья решений, где в каждой вершине объекты делятся по одному значению одного признака (*threshold*). На деревья *масштабирование* не влияет.\n",
    "\n",
    "4. **Логистическая регрессия без регуляризации** - модель классификации, которая разделяет объекты гиперплоскостью. \n",
    "Формула нахождения принадлежности классу в бинарной классификации:\n",
    "$$\n",
    "\\widehat{y}(x)= sign\\left(w^{T}x+w_{0}\\right)\n",
    "$$\n",
    "Функция потерь *logloss*:\n",
    "$$\n",
    "L(w) = - \\sum_{n=1}^N \\mathbb{I}[y_n = +1]\\cdot\\ln{\\sigma(w^{T}x_n+w_0))} + \\mathbb{I}[y_n = -1]\\cdot\\ln{(1-\\sigma(w^{T}x_n+w_0))} \\rightarrow \\min_w\n",
    "$$\n",
    "*Discriminant function*:\n",
    "$$\n",
    "score(y=+1|x)=w^{T}x + w_0 = g(x)\n",
    "$$\n",
    "*Вероятность принадлежности классу*:\n",
    "$$\n",
    "p(y=+1|x)=\\sigma(w^{T}x + w_0)\n",
    "$$\n",
    "*Сигмоид*:\n",
    "$$\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "*Масштабирование* ускорит градиентный спуск и ограничит влияние определенных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explain idea of weighted K-NN. Give examples of weights. Will feature scaling affect predictions of K-NN? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Взвешанный K-NN**: среди ближайших соседей приоритетный класс (значение) выбирается с учетом весов:\n",
    "* Классификация:\n",
    "$$\n",
    "\\begin{align*}\n",
    "g_{c}(x) & =\\sum_{k=1}^{K}w(k,\\,\\rho(x,x_{i_{k}}))\\mathbb{I}[y_{i_{k}}=c],\\quad c=1,2,...C.\\\\\n",
    "\\widehat{y}(x) & =\\arg\\max_{c}g_{c}(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "* Регрессия\n",
    "$$\n",
    "\\widehat{y}(x)=\\frac{\\sum_{k=1}^{K}w(k,\\,\\rho(x,x_{i_{k}}))y_{i_{k}}}{\\sum_{k=1}^{K}w(k,\\,\\rho(x,x_{i_{k}}))}\n",
    "$$\n",
    "\n",
    "**Примеры весов**:\n",
    "1. Веса, зависящие от индекса:\n",
    "$$\n",
    "w_{k}=\\alpha^{k},\\quad\\alpha\\in(0,1)\n",
    "$$\n",
    "$$\n",
    "w_{k}=\\frac{K+1-k}{K}\n",
    "$$\n",
    "2. Веса, зависящие от расстояния, где \n",
    "$z_i$ - $i$-ый ближайший сосед: \n",
    "$$\n",
    "w_{k}=\\begin{cases}\n",
    "\\frac{\\rho(z_{K},x)-\\rho(z_{k},x)}{\\rho(z_{K},x)-\\rho(z_{1},x)}, & \\rho(z_{K},x)\\ne\\rho(z_{1},x)\\\\\n",
    "1 & \\rho(z_{K},x)=\\rho(z_{1},x)\n",
    "\\end{cases}\n",
    "$$\n",
    "$$\n",
    "w_{k}=\\frac{1}{\\rho(z_{k},x)}\n",
    "$$\n",
    "\n",
    "*Масштабирование* сильно влияет на предсказания модели, так как позволяет каждому признаку в равной степени влиять на прогноз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Give definition discriminant functions and margin for classification. What is its intuition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discriminant function** - функция, с помощью которой принимается решения в задачах классификации.\n",
    "$$\n",
    "c=\\arg\\min_{i}g_c \\text{ или } c=\\arg\\max_{i}g_c\n",
    "$$\n",
    "**Margin** - оценка того, насколько верно мы предсказали класс для объекта. (расстояние от объекта до гиперплоскости)\n",
    "\n",
    "*Margin* в линейной классификации:\n",
    "$$\n",
    "M(x,y) =y\\cdot\\left(w^{T}x+w_{0}\\right)\n",
    "$$\n",
    "$$\n",
    "y \\in \\{-1,+1\\}\n",
    "$$\n",
    "$M(x,y|w)>0 \\Leftrightarrow$ объект верно классифицирован\n",
    "\n",
    "<img src=\"img/margin.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Definition of decision tree. Definition of impurity function. Examples of impurity function. Splitting rule selection for CART trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решающее дерево** - модель, совершающая прогнозы на основе дереве $T$ (ориентированного, связного графа без циклов).\n",
    "\n",
    "**Типы веришн**:\n",
    "1. Корень дерева\n",
    "2. Внутрениие веришны, каждая из которых имеет $\\ge2$ детей (задается параметром $K$)\n",
    "3. Листья, которые не имеют дочерних узлов, но имеют связанные значения прогнозирования\n",
    "\n",
    "\n",
    "- Каждая нетерминальная вершина $t$ связана с *функцией проверки* (*check-fuction*) $Q_{t}(x)$\n",
    "- Каждое ребро $r_{t}(1),...r_{t}(K_{t})$ связано с набором значений *функции проверки* $Q_{t}(x)$: $S_{t}(1),...S_{t}(K_{t})$ таким, что:\n",
    "    - $\\bigcup_{k}S_{t}(k)=range[Q_{t}]$\n",
    "    - $S_{t}(i)\\cap S_{t}(j)=\\emptyset$  $\\forall i\\ne j$\n",
    "    \n",
    "**Критерий информативности** (*Impurity function*) - функция, на основе которой осуществляется разбиение выборки в каждой вершине. Для случая классификации функция $\\phi(t)=\\phi(p_{1},p_{2},...p_{C})$, где $p_{1},...p_{C}$ -вероятность классов в вершине $t$, должна удовлетворять следующим условиям:\n",
    "1. $\\phi$ определена для $p_j \\ge 0$ и  $\\sum_{j}p_{j}=1$\n",
    "2. $\\phi$ достигает максимума при $p_j = 1/C$\n",
    "3. $\\phi$ достигает минимума $\\exists j:\\,p_{j}=1,\\,p_{i}=0$ $\\forall i\\ne j$\n",
    "4. $\\phi$ симметричная функция для $p_{1},p_{2},...p_{C}$\n",
    "\n",
    "**Примеры критериев информативности для случая классификации**:\n",
    "1. *Критерий Джини* (*Gini criterion*) - вероятность сделать ошибку, когда случайно предсказываешь класс с веротностями классов $[p(\\omega_{1}|t),...p(\\omega_{C}|t)]$: \n",
    "$$\n",
    "I(t)=\\sum_{i}p(\\omega_{i}|t)(1-p(\\omega_{i}|t))=1-\\sum_{i}[p(\\omega_{i}|t)]^{2}\n",
    "$$\n",
    "2. *Энтропия* (*Entropy*) - мера неопределенности случайной величины:\n",
    "$$\n",
    "I(t)=-\\sum_{i}p(\\omega_{i}|t)\\ln p(\\omega_{i}|t)\n",
    "$$\n",
    "3. Ошибка классификации (*Classification error*) - частота ошибок при классификации преобладающим классом:\n",
    "$$\n",
    "I(t)=1-\\max_{i}p(\\omega_{i}|t)\n",
    "$$\n",
    "\n",
    "**Примеры критериев информативности для случая регрессии:**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi(t) & =\\frac{1}{K}\\sum_{i\\in I}\\left(y_{i}-\\mu\\right)^{2}\\quad \\text{(MSE)}\\\\\n",
    "\\phi(t) & =\\frac{1}{K}\\sum_{i\\in I}|y_{i}-\\mu|\\quad \\text{(MAE)}\n",
    "\\end{align*}\n",
    "$$\n",
    "где $I=\\{i_{1},...i_{K}\\}$ - объекты, попадающие в вершину $t$, а $\\mu$ - среднее  или медиана всех $y_i$\n",
    "\n",
    "**Выбор критерия разбиения (splitting rule selection):**\n",
    "- $\\Delta I(t)$ - мера качества разбиения вершины $t$ в дочерние вершины $t_{1},...t_{C}$ (в случае, если используется энтропия, то называется *information gain*)\n",
    "$$\n",
    "\\Delta I(t)=I(t)-\\sum_{i=1}^{C}I(t_{i})\\frac{N(t_{i})}{N(t)}\n",
    "$$\n",
    "$$\n",
    "\\Delta I(t)=I(t)-\\left(I(t_{L})\\frac{N(t_{L})}{N(t)} + I(t_{R})\\frac{N(t_{R})}{N(t)}\\right)\n",
    "$$\n",
    "\n",
    "**CART оптимизация:** выбор признака $i_t$ и порога (*threshold*) $h_t$, который максимизирует $\\Delta I(t)$\n",
    "$$\n",
    "i_{t},\\,h_{t}=\\arg\\max_{k,h}\\Delta I(t)\n",
    "$$\n",
    "И получаются новые вершины:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\text{left child }t_{1}, & \\text{if }x^{i_{t}}\\le h_{t}\\\\\n",
    "\\text{right child }t_{2}, & \\text{if }x^{i_{t}}>h_{t}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Propose stopping rules for setting tree node to leaf node based on: class distribution, number of samples in the leaf, impurity function, change in impurity function from splitting this node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Распределение класса**: если в вершине частота элементов одного класса больше некоторого значения, то вершина становится листом;\n",
    "2. **Количество элемнтов в листе**: если количество элементов в вершине меньше некоторого значения, то вершина становится листом;\n",
    "3. **Критерий информативности**: если критерий информативности в веришне меньше определееного значения, то вершина становится листом;\n",
    "4. **Изменение критерия информативности после разбиения вершины**: если разница критериев информативности у вершины и ее дочерней вершины меньше определенного значения, то дочерняя вершина становится листом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Linear regression estimated with ordinary least squares - derive its solution. RIDGE and LASSO regularizations - write them out. Which of them selects features and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**МНК:**\n",
    "$$\n",
    "L(\\beta_0,\\beta_1,\\dots) = \\frac{1}{2N}\\sum^{N}_{n=1}(\\hat{y}_{n} - y_{n})^2 = \\frac{1}{2N}\\sum^{N}_{n=1}\\left(\\sum_{d=0}^{D}\\beta_{d}x_{n}^{d}-y_{n}\\right)^{2}  \\rightarrow \\min\\limits_{\\beta}\n",
    "$$\n",
    "В матричной форме:\n",
    "$$\n",
    "L(\\beta) = \\frac{1}{2N}(\\hat{y} - y)^{\\top}(\\hat{y} - y) = \\frac{1}{2N}(X\\beta - y)^{\\top}(X\\beta - y) \\rightarrow \\min\\limits_{\\beta}\n",
    "$$\n",
    "Раскроем:\n",
    "$$\n",
    "\\begin{align*} \n",
    "L(\\beta) & = \\frac{1}{2N}(X\\beta - y)^{\\top}(X\\beta - y) \\\\\n",
    "         & = \\frac{1}{2N}\\left( \\beta^\\top X^\\top X \\beta - 2 (X\\beta)^\\top y + y^\\top y \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "Правила производных (напоминания):\n",
    "$$\n",
    "\\begin{array}{rcl} \\frac{\\partial}{\\partial x} x^T a &=& a \\\\ \\frac{\\partial}{\\partial x} x^T A x &=& \\left(A + A^T\\right)x \\\\ \\frac{\\partial}{\\partial A} x^T A y &=& xy^T \\end{array}\n",
    "$$\n",
    "Возьмем производную:\n",
    "$$ \n",
    "\\nabla L(\\beta) = \\left(\\frac{\\partial L(\\beta)}{\\partial\\beta_i} \\right)_{i=0\\dots d}  = \\frac{1}{2N}\\left(2X^\\top X \\beta -2X^\\top y\\right) = 0 \n",
    "$$\n",
    "\n",
    "$$\n",
    "X^{\\top}X\\beta-X^{T}y = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta = (X^\\top X)^{-1} X^\\top y \\quad\\text{(Normal Equation)}\n",
    "$$\n",
    "\n",
    "**Регуляризация:**\n",
    "$$\n",
    "\\sum_{n=1}^{N}\\left(x_{n}^{T}\\beta-y_{n}\\right)^{2}+\\lambda R(\\beta)\\to\\min_{\\beta}\n",
    "$$\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{Lasso regression} & R(\\beta)=||\\beta||_{1} \\\\\n",
    "\\text{Ridge regression} & R(\\beta)=||\\beta||_{2}^{2} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "*Дополнительно: Аналитическое решение Ridge регрессии*\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N}\\left(x_{n}^{T}\\beta-y_{n}\\right)^{2}+\\lambda\\beta^{T}\\beta\\to\\min_{\\beta}\n",
    "$$\n",
    "$$\n",
    "\\begin{gathered}2\\sum_{n=1}^{N}x_{n}\\left(x_{n}^{T}\\beta-y_{n}\\right)+2\\lambda\\beta=0\\\\\n",
    "2X^{T}(X\\beta-y)+\\lambda\\beta=0\\\\\n",
    "\\left(X^{T}X+\\lambda I\\right)\\beta=X^{T}y\n",
    "\\end{gathered}\n",
    "$$\n",
    "И в итоге:\n",
    "$$\n",
    "\\widehat{\\beta}=(X^{T}X+\\lambda I)^{-1}X^{T}y\n",
    "$$\n",
    "\n",
    "Основное отличие *лассо* и *гребневой* регрессий заключается в том, что первая может приводить к обращению некоторых независимых переменных в ноль, тогда как вторая уменьшает их до значений, близких к нулю.\n",
    "\n",
    "Рассмотрим для простоты двумерное пространство независимых переменных. В случае лассо регрессии органичение на коэффициенты представляет собой ромб ($|\\beta_1| + |\\beta_2| \\le t$), в случае гребневой регрессии — круг ($\\beta_1^2 + \\beta_2^2 \\le t^2$). Необходимо минимизировать функцию ошибки, но при этом соблюсти ограничения на коэффициенты. С геометрической точки зрения задача состоит в том, чтобы найти точку касания линии, отражающей функцию ошибки с фигурой, отражающей ограничения на $\\beta$. Из рисунка интуитивно понятно, что в случае лассо регрессии эта точка с большой вероятностью будет находиться на углах ромба, то есть лежать на оси, тогда как в случае гребневой регрессии такое происходит очень редко. Если точка пересечения лежит на оси, один из коэффициентов будет равен нулю, а значит, значение соответствующей независимой переменной не будет учитываться.\n",
    "\n",
    "<img src=\"img/reg.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Definition of linear classifier for two and multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Линейная классификация** - модель классификации, которая разделяет объекты гиперплоскостью. \n",
    "Формула нахождения принадлежности классу в бинарной классификации:\n",
    "$$\n",
    "\\widehat{y}(x)= sign\\left(w^{T}x+w_{0}\\right)\n",
    "$$\n",
    "\n",
    "**Margin (Отступ)** показывает то, насколько верно мы предсказали класс для объекта (расстояние от объекта до гиперплоскости).\n",
    "\n",
    "*Margin* в линейной классификации:\n",
    "$$\n",
    "M(x,y) =y\\cdot\\left(w^{T}x+w_{0}\\right)\n",
    "$$\n",
    "$$\n",
    "y \\in \\{-1,+1\\}\n",
    "$$\n",
    "$M(x,y|w)>0 \\Leftrightarrow$ объект верно классифицирован\n",
    "\n",
    "Цель модели: минимизировать *misclassification rate*:\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{n=1}^{N}\\mathbb{I}[M(x_{n},y_{n}|w)<0]\\to\\min_{w}\n",
    "$$\n",
    "Для того, чтобы эффективнее искать минимум вводятся функции $\\mathcal{L}(M)$ такие, что $\\mathbb{I}[M]\\le\\mathcal{L}(M)$, и их можно дифферинцировать. \n",
    "$$\n",
    "\\begin{gathered}\\begin{gathered}\\text{MISCLASSIFICATION RATE}\\end{gathered}\n",
    "=\\frac{1}{N}\\sum_{n=1}^{N}\\mathbb{I}[M(x_{n},y_{n}|w)<0]\\\\\n",
    "\\le\\frac{1}{N}\\sum_{n=1}^{N}\\mathcal{L}(M(x_{n},y_{n}|w))=L(w)\n",
    "\\end{gathered}\n",
    "$$\n",
    "В случае многоклассовой классификации используется два подхода:\n",
    "1. Подход **One-vs-Rest** принимает один класс как положительный, а остальные все как отрицательный и обучает классификатор. Таким образом, для данных, имеющих $n$ классов, он обучает $n$ классификаторов. На этапе классификации каждый классификатор предсказывает вероятность того, что выбран определенный класс, и выбирается класс с наибольшей вероятностью.\n",
    "2. **One-vs-One** рассматривает каждую пару классов и обучает классификатор на подмножестве данных, содержащих эти классы. Тренируется всего $n(n-1)/2$ классификаторов. На этапах классификации каждый классификатор прогнозирует один класс. (Это отличает от *One-vs-Rest*, где каждый классификатор предсказывает вероятность). И класс, который был предсказан больше всего раз, является ответом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Show that for linearly dependent features vector of coefficients of linear regression is not uniquely defined. What is dummy variable trap? Given two linearly dependent features which of them will be eliminated by LASSO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Доказательство неопределенности $\\widehat{\\beta}$, если присутствуют линейно-зависимые признаки:**\n",
    "\n",
    "Допустим у нас есть линейно-зависимые признаки: $\\exists\\alpha:\\,x^{T}\\alpha=0\\,\\forall x$\n",
    "\n",
    "А так же вектор $\\beta$ - решение линейной регрессии $y=x^{T}\\beta$.\n",
    "\n",
    "Тогда: $x^{T}\\beta\\equiv x^{T}\\beta+kx^{T}\\alpha\\equiv x^{T}(\\beta+k\\alpha)$, поэтому $\\beta+k\\alpha$ тоже решение! Значит, вектор $\\widehat{\\beta}$ - не определен однозначно.\n",
    "\n",
    "**Dummy varable trap** - случай, при котором при one-hot-encoding признак разбивается на то же количество бинарых признаков, что и значений в данном признаке. При этом возникает линейная зависимость между данным dummy переменными (бинарными признаками), что приводит к неопределенности ответа.\n",
    "\n",
    "Среди двух линейно-зависимых признаков Лассо удалит тот, у которого масштаб меньше, так как признаку с бОльшим масштабом будет необходим меньший коэффициент. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. How to estimate parameters of linear classifier? Write out the optimization task for different loss functions. Compare qualitatively typical loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
